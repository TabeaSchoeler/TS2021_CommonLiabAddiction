out=as.data.frame(cbind(d, var.d,  l.d, u.d, N.total, pval.d))
print(out)
return(out)
}
# Estimate effective sample size
# The sample size needed in within-designs (NW) relative to the sample needed in between-designs (NB), assuming normal distributions, is (from Maxwell & Delaney, 2004, p. 561, formula 45):
n_effective_dependent=function(n_within, r){
# Original formula: n_within=2*n_between/(-r+1), taken from https://www.mathpapa.com/calc.html?q=4x+7%3D2x+1
# As follows when folved for n_within
n_between=2*n_within*(1+r) # Note: When r=0 (ie complete independence, then the within subject number is double the between subject)
return(  round(n_between,0) )
}
vltovd <- function(vl) {
vd = vl * (3/pi^2)
return(vd)
}
vdtovl <- function(vd) {
vl = vd*(pi^2/3)
return(vl)
}
vrtovd <- function(vr,r) {
vd = (4*vr)/(1-r^2)^3
return(vd)
}
#Convert d to r
dtor <- function(n1,n2,d) {
a = (n1+n2)^2/(n1*n2)
r = d/(sqrt(d^2+a))
return(r)
}
vdtovr <- function(n1,n2,vd,d) {
a = (n1+n2)^2/(n1*n2)
vr = a^2*vd/(d^2+a)^3
return(vr)
}
# Get_ncp (used for repated t-test)
get_ncp_t <- function(t, df_error, conf.level = 0.95) {
alpha <- 1 - conf.level
probs <- c(alpha / 2, 1 - alpha / 2)
if (isTRUE(all.equal(t, 0))) {
t_ncp <- qt(probs, df_error)
return(t_ncp)
}
ncp <- suppressWarnings(optim(
par = 1.1 * rep(t, 2),
fn = function(x) {
p <- pt(q = t, df = df_error, ncp = x)
abs(max(p) - probs[2]) +
abs(min(p) - probs[1])
},
control = list(abstol = 1e-09)
))
t_ncp <- unname(sort(ncp$par))
if (isTRUE(all.equal(t_ncp[1], 0))) {
t_ncp[1] <- qt(probs[1], df_error)
}
if (isTRUE(all.equal(t_ncp[2], 0))) {
t_ncp[2] <- qt(probs[2], df_error)
}
return(t_ncp)
}
# convert t-test value from repeated measures to Cohen d
convertT_dependent = function(t, df, n) {
d = t / sqrt(df) # Amended from package 'effectsize', https://github.com/easystats/effectsize/blob/master/R/convert_tFz_to_d.R
t_ncp <- get_ncp_t(t, df) # NCP (noncentrality parameter) method: positions the observed ncp at the 0.025 quantile at the 0.975 quantile. Then, the interval between these two distributions’ ncp is taken to form the upper and lower bound of the 95% CI respectively.
l.d <- t_ncp[1] / sqrt(df) # REF: A review of effect sizes and their con1dence intervals, Part I: The Cohen’s d family
u.d <- t_ncp[2] / sqrt(df)
se.d = (u.d-l.d)/(2*1.96) # calculate the standard error
var.d= se.d^2
z = d/se.d
pval.d=2*pnorm(-abs(z))
N.total=n
out=as.data.frame(cbind(d, var.d,  l.d, u.d, N.total, pval.d))
return(out)
}
convertT_dependent(t=5, df=1, n=52)
# convert t-test value from repeated measures to Cohen d (provides similar results to convertT_dependent)
convertT_repeated=function(t, df, r, n) {
d= t / sqrt(df) # REF: Calculating and reporting effect sizes to facilitate cumulative science : a practical primer for t-tests and ANOVAs, DOI: 10.3389/fpsyg.2013.00863
var.d=(2*(1-r)/n)+(d/(2*n-2)) # REF: http://methods.sagepub.com/Reference//encyc-of-research-design/n58.xml?PageNum=185
#var.d<- ((1/n) + ((d^2)/(2*n)))*2*(1-r) # where r = cor of pre and post test correlation
# var.d=(2*(1-r)/n)+ d^2/ (2*n)
l.d=d-1.96 * sqrt(var.d) # sqrt(var.d) = SE
u.d=d+1.96 * sqrt(var.d)
se.d = (u.d-l.d)/(2*1.96) # calculate the standard error
z = d/se.d
pval.d=2*pnorm(-abs(z))
N.total=n
out=as.data.frame(cbind(d, var.d,  l.d, u.d, N.total, pval.d))
return(out)
}
# convert t-test value from repeated measures to Cohen d
convertF_dependent = function(f, df, alpha=0.05, n) {
if (df > 1) {
stop("Cannot convert F with more than 1 df to (partial) r.")
}
t=sqrt(f)
d = t / sqrt(df)
alpha <- alpha
probs <- c(alpha / 2, 1 - alpha / 2) # Amended from package 'effectsize', https://github.com/easystats/effectsize/blob/master/R/convert_tFz_to_d.R
t_ncp <- qt(probs, df) # NCP (noncentrality parameter) method: positions the observed ncp at the 0.025 quantile at the 0.975 quantile. Then, the interval between these two distributions’ ncp is taken to form the upper and lower bound of the 95% CI respectively.
l.d <- t_ncp[1] / sqrt(df) # REF: A review of effect sizes and their con1dence intervals, Part I: The Cohen’s d family
u.d <- t_ncp[2] / sqrt(df)
se.d = (u.d-l.d)/(2*1.96) # calculate the standard error
var.d= se.d^2
z = d/se.d
pval.d=2*pnorm(-abs(z))
N.total=n
out=as.data.frame(cbind(d, var.d,  l.d, u.d, N.total, pval.d))
return(out)
}
r_to_d=function(r) {
d = 2 * r / sqrt(1 - r^2) # Taken from  r package 'effectsize'
return(d)
}
# Confidence intervals from odds ratio to p-value
or_to_p=function(or, l.or, u.or) {
# REF: https://www.bmj.com/content/343/bmj.d2304
se = (log(u.or)-log(l.or))/(2*1.96) # calculate the standard error
z = log(or)/se
p=2*pnorm(-abs(z))
return(p)
}
# Get proportion table
chiSquareCalc=function(NriskA_case, NriskB_case, NriskA_control, NriskB_control, labelA, labelB){
datTable <- matrix(c(NriskA_case,NriskB_case,NriskA_control,NriskB_control), ncol=2)
colnames(datTable) <- c('case', 'control')
rownames(datTable) <- c(labelA, labelB)
print(as.table(datTable))
proportions= CrossTable(datTable)$prop.row[,1]
print("Proportion of cases in each group")
print(proportions)
chiSquEstimate=as.numeric(chisq.test(datTable, correct=FALSE)$statistic) # correct=FALSE, turn off Yates’ continuity correction.
chiSquEstimate_p=as.numeric(chisq.test(datTable, correct=FALSE)$p.value)
Ntot=sum(datTable)
NCases=sum(as.data.frame(datTable)[,1])
out=cbind(chiSquEstimate, Ntot, NCases, chiSquEstimate_p)
print(out)
return(out)
}
chiSquareCalc_repeated = function(percCan, percPLB, nTot, r){
# Derive effective sample size
n_effective=n_effective_dependent(nTot, r=r)
# Estimate number of cases with adverse responses to cannabis vs. no adverse responses to cannabis
n_can=n_effective/2
n_PLB=n_effective/2
casesCan=round((n_can/100)*percCan,0)
healthyCan=round(n_can-casesCan,0)
casesPLB=round((n_PLB/100)*percPLB,0)
healthyPLB=round(n_PLB-casesPLB,0)
# Get chi square estimates
datTable <- matrix(c(casesCan,casesPLB,healthyCan,healthyPLB), ncol=2)
colnames(datTable) <- c('case', 'healthy')
rownames(datTable) <- c("Cannabis", "Placebo")
print(as.table(datTable))
proportions= CrossTable(datTable)$prop.row[,1]
print("Proportion of cases in each group")
print(proportions)
chiSquEstimate=as.numeric(CrossTable(datTable, chisq = T)$chisq$statistic)
chiSquEstimate_p=as.numeric(CrossTable(datTable, chisq = T)$chisq$p.value)
#chiSquEstimate2=as.numeric(chisq.test(datTable, correct=FALSE)$statistic)
#chiSquEstimate_p=as.numeric(chisq.test(datTable, correct=FALSE)$p.value)
Ntot=sum(datTable)
NCases=sum(as.data.frame(datTable)[,1])
out=cbind(chiSquEstimate, Ntot, NCases, chiSquEstimate_p)
print(out)
}
# mcnemar.test when estimating chi-square from dependent samples
mcnemarCalc=function(NriskA_case, NriskB_case, NriskA_control, NriskB_control, labelA, labelB){
datTable <- matrix(c(NriskA_case,NriskB_case,NriskA_control,NriskB_control), ncol=2)
colnames(datTable) <- c('case', 'control')
rownames(datTable) <- c(labelA, labelB)
print(as.table(datTable))
proportions= CrossTable(datTable)$prop.row[,1]
print("Proportion of cases in each group")
print(proportions)
mcnemar.test(proportions)
chiSquEstimate=as.numeric(mcnemar.test(prop.table(datTable))$statistic)
chiSquEstimate_p=as.numeric(mcnemar.test(prop.table(datTable))$p.value)
Ntot=sum(datTable)
NCases=sum(as.data.frame(datTable)[,1])
out=cbind(chiSquEstimate, Ntot, NCases, chiSquEstimate_p)
print(out)
return(out)
}
# Generate data when prevalences were reported
genDat=function(n_cases, n_tot){
dat1=c(rep(0,n_tot-n_cases), rep(1,n_cases))
dat2=rep(0,n_tot)
m1=mean(dat1)
m2=mean(dat2)
sd1=sd(dat1)
sd2=sd(dat2)
out=data.frame(m1, m2, sd1, sd2, n_tot)
print(out)
return(out)
}
# == Hallucinations
# Convert CI to SD
zuurman2008_8mgTHCvaped_hallucinations_beta=0.144
zuurman2008_8mgTHCvaped_hallucinations_sd=(0.266-0.021)/(2*1.96)*sqrt(zuurman2008_8mgTHCvaped_n)
zuurman2008_8mgTHCvaped_hallucinations_sd
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# ========================== Melges (1976)  =============================
# ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# REF: Melges, F. T. "Tracking difficulties and paranoid ideation during hashish and alcohol intoxication." The American journal of psychiatry 133, no. 9 (1976): 1024-1028.
melges_paranoia_20mgTHC_n=6
# Derive cohen d (0.015 mg/kg)
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=5.8,
m2=-0.2,
sd1=2,
sd2=2,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
# Derive cohen d (0.015 mg/kg)
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=5.8,
m2=-0.2,
sd1=4,
sd2=4,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
# Derive cohen d (0.015 mg/kg)
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=5.8,
m2=-0.2,
sd1=5,
sd2=5,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
# Derive cohen d (0.015 mg/kg)
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=5.8,
m2=-0.2,
sd1=8,
sd2=8,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
# Derive cohen d (0.015 mg/kg)
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=5.8,
m2=-0.2,
sd1=9,
sd2=9,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
melges_paranoia_20mgTHC_fast
# Derive cohen d
melges_paranoia_20mgTHC_fast=d_meanDiff_dependent(m1=4,
m2=-0.2,
sd1=9,
sd2=9,
n=melges_paranoia_20mgTHC_n,
r=r_withinSubj_between_timepoints)
melges_paranoia_20mgTHC_fast
#!/usr/bin/env Rscript
HOME="/lustre/scratch/scratch/ucjucho/TS2020_GWAcannabisSensitivity"
#!/usr/bin/env Rscript
extract="COGA"
drop_download(
local_path = paste0(HOME,"/data/CanSensIndividual.xlsx"),
path = paste0(LOCAL,"/data/CanSensIndividual.xlsx"), overwrite=TRUE)
print("Read in excel file from cluster")
CanSensIndividualAll=read.xlsx(paste0(HOME,"/data/CanSensIndividual.xlsx"))
source(paste0(HOME, "/analysis/input.R"))
print(HOME)
source(paste0(HOME, "/analysis/input.R"))
# Hallucinations (effect of single dose, no means/sd reported for this analysis)
tableChi=chiSquareCalc_repeated(percCan= (6/noyesCancerTHC_n)*100, percPLB = 0, nTot =  noyesCancerTHC_n, r=r_withinSubj_between_timepoints)
noyesCancerTHC_n=44
# Hallucinations (effect of single dose, no means/sd reported for this analysis)
tableChi=chiSquareCalc_repeated(percCan= (6/noyesCancerTHC_n)*100, percPLB = 0, nTot =  noyesCancerTHC_n, r=r_withinSubj_between_timepoints)
tableChi
chiSquEstimate_noyesCancerTHC_hallucinations=tableChi[1]
Ntot_noyesCancerTHC_hallucinations=tableChi[2]
Ntot_noyesCancerTHC_hallucinations
# Get cohen d
extract_noyesCancerTHC_hallucinations=chies(chiSquEstimate_noyesCancerTHC_hallucinations, Ntot_noyesCancerTHC_hallucinations, dig=50)
extract_noyesCancerTHC_hallucinations
# Rate
noyesCancerTHC_n_hallucinationsDF=data.frame(event=6, n=noyesCancerTHC_n)
#  placebo, 10 and 20 mgofTHC, and
noyesCancerTHC_n=44
# Hallucinations (effect of single dose, no means/sd reported for this analysis)
tableChi=chiSquareCalc_repeated(percCan= (6/noyesCancerTHC_n)*100, percPLB = 0, nTot =  noyesCancerTHC_n, r=r_withinSubj_between_timepoints)
chiSquEstimate_noyesCancerTHC_hallucinations=tableChi[1]
Ntot_noyesCancerTHC_hallucinations=tableChi[2]
# Get cohen d
extract_noyesCancerTHC_hallucinations=chies(chiSquEstimate_noyesCancerTHC_hallucinations, Ntot_noyesCancerTHC_hallucinations, dig=50)
pEstimate_noyesCancerTHC_hallucinations=data.frame(pEstimate=NA, Ntot=noyesCancerTHC_n)
# Rate
noyesCancerTHC_hallucinationsDF=data.frame(event=6, n=noyesCancerTHC_n)
extract_noyesCancerTHC_hallucinations
source(paste0(HOME, "/analysis/input.R"))
source(paste0(HOME, "/analysis/input.R"))
marcusCanUserQuest_n=43 + 38 # females plus males
marcusCanUserQuest_nFemale=43
marcusCanUserQuest_hallucinations=1 # only assessed in femals
marcusCanUserQuest_paranoid=2 + 2 # assessed in males and females
# Rate
marcus1974_hallucinationsDF=data.frame(event=marcus1974CanUserQuest_hallucinations, n=marcus1974CanUserQuest_nFemale)
marcus1974CanUserQuest_n=43 + 38 # females plus males
marcus1974CanUserQuest_nFemale=43
marcus1974CanUserQuest_hallucinations=1 # only assessed in femals
marcus1974CanUserQuest_paranoid=2 + 2 # assessed in males and females
# Rate
marcus1974_hallucinationsDF=data.frame(event=marcus1974CanUserQuest_hallucinations, n=marcus1974CanUserQuest_nFemale)
marcus1974_hallucinationsDF
marcus1974CanUserQuest_n=43 + 38 # females plus males
marcus1974CanUserQuest_nFemale=43
marcus1974CanUserQuest_hallucinations=1 # only assessed in femals
marcus1974CanUserQuest_paranoid=2 + 2 # assessed in males and females
# Rate
marcus1974_hallucinationsDF=data.frame(event=marcus1974CanUserQuest_hallucinations, n=marcus1974CanUserQuest_nFemale)
# Rate
marcus1974_paranoiaDF=data.frame(event=marcus1974CanUserQuest_paranoid, n=marcus1974CanUserQuest_n)
datMeta_all=read.csv(paste0(HOME, "/data/Data_extraction_26_02_2021.csv"),
header=T,
na.strings=c("","NA"))
marcus1974_paranoiaDF
marcus1974CanUserQuest_hallucinations/marcus1974CanUserQuest_nFemale
marcus1974CanUserQuest_paranoid/marcus1974CanUserQuest_n
# remove all of the objects that are stored in the global environment
rm(list = ls())
.libPaths()
.libPaths("~/Dropbox/progs/R/library")
# Load and install libraries
.libPaths( c( .libPaths(), "~/Dropbox/progs/R/library") )
load.lib=c('pheatmap', 'ggthemes', 'ggpubr', 'Qtlizer', 'devtools', 'GenomicSEM', 'semPlot', 'data.table', 'Matrix',
'sem', 'Matrix', 'stats', 'semTools', 'ggcorrplot', 'grex', 'openxlsx', 'rJava', 'qqman', 'gmodels', 'ggplot2',
'ggcorrplot', 'tidyverse', 'reshape2', 'pdftools', 'plyr', 'magick', 'phenoscanner', 'VennDiagram','ggpubr',
'biomaRt', 'viridis', 'rdrop2', 'lavaan', 'lavaanPlot', 'DiagrammeRsvg', 'DiagrammeR', 'gprofiler2', 'knitr',
'gridExtra', 'CMplot', 'configr', 'purrr', 'TwoSampleMR', 'GOexpress', 'ieugwasr', 'xlsx')
install.lib<-load.lib[!load.lib %in% installed.packages()]
# Install missing packages
for(lib in install.lib) install.packages(lib, dependencies=TRUE)
# Load all packages
sapply(load.lib,require,character=TRUE)
# Set directory path
HOME="/Users/tabea/Dropbox/githubProjects/TabeaSchoeler/TS2020_GenomicSEM"
# Function to Merge as rbind
mergeLongFormat=function(list, label, cleanLabel){
dataframe = mapply(`[<-`, list, 'Label', value = label, SIMPLIFY = FALSE)
dataframeOut=dplyr::bind_rows(dataframe)
if (cleanLabel==TRUE) {
dataframeOut$Label=ifelse(duplicated(dataframeOut$Label)==FALSE, as.character(dataframeOut$Label), " ")
return(dataframeOut)
} else {
return(dataframeOut)
}
}
# Derive function to relabel
recodeName=function(data){
data=revalue(data, c("commonLiability"="Common liability",
"F1"="Common liability",
"commonLiability_filtered"= "Common liability",
"commonLiability_cancigalc_corRes" = "Common liability",
"commonLiability_filtered_cancigalc_corRes" = "Common liability (Qsnp filtered)",
"CigaretteDependency" = "Cigarette (dependency)",
"AlcoholDependency" = "Alcohol (dependency)",
"DrinksPerWeek" = "Alcohol (frequency)",
"CigarettesPerDay" = "Cigarette (frequency)",
"CannabisUseFrequency"= "Cannabis (frequency)",
"CannabisUseDisorder"= "Cannabis (dependency)"))
return(data)
}
recodeNameBreak=function(data){
data=revalue(data, c("commonLiability"="Common \n liability",
"F1"="Common \n liability",
"commonLiability_filtered"= "Common \n liability",
"CigaretteDependency" = "Cigarette \n (dependency)",
"AlcoholDependency" = "Alcohol \n (dependency)",
"DrinksPerWeek" = "Alcohol \n (frequency)",
"CigarettesPerDay" = "Cigarette \n (frequency)",
"CannabisUseFrequency"= "Cannabis \n (frequency)",
"CannabisUseDisorder"= "Cannabis \n (dependency)"))
return(data)
}
# Rename label (add line breaks)
swr = function(string, nwrap) {
paste(strwrap(string, width=nwrap), collapse="\n")
}
swr = Vectorize(swr)
# Check of order makes sense
orderLabels=function(variable){
variableOut <- factor(variable,
levels = c("commonLiability",
"commonLiability_filtered",
"CigarettesPerDay",
"CigaretteDependency",
"DrinksPerWeek" ,
"AlcoholDependency",
"CannabisUseFrequency",
"CannabisUseDisorder"))
return(variableOut)
}
# Filter SNPs based on Q-statistic
filterQtest=function(df, name){
print(paste0("Iteration: for ",name))
if (name=="commonLiability") {
print("Common factor")
df=subset(df,   Q_chisq_pval >= 5e-8)
} else {
print("Other substance use phenotypes")
df=subset(df,   Q_chisq_pval < 5e-8)
}
return(df)
}
# Select top SNPs according to p-value
topSNPs=function(data, NtopSNPs, P=P){
data=data %>% top_n(NtopSNPs, -(as.numeric(P)))
return(data)
}
# Function to format p-values and number with many decimals
formatNum=function(vecP=NULL, vecNum=NULL, decimal=3, df){
if(length(vecP)!=0){
for (i in 1:length(vecP)) {
print(paste0("Format ",vecP[i]))
df[[vecP[i]]]=formatC(df[[vecP[i]]], digits=decimal) # Format p-values
}
}
if(length(vecNum)!=0){
for (i in 1:length(vecNum)) {
print(paste0("Round ",vecNum[i]))
df[[vecNum[i]]]=round( df[[vecNum[i]]], decimal)
}
}
return(df)
}
# Order list according to phenotype list
reorderPheno=function(listIn, listOut=list()){
for(i in 1:length(phenoNames)){
print(phenoNames[i])
listOut[[i]] = listIn[[phenoNames[i]]]
}
names(listOut)=phenoNames
return(listOut)
}
filterGenesEQTL=function(df){
# Remove duplicates (eQTLs are linked to different tissues, but do not need to include here)
df_out=subset(df,   duplicated(paste0(df$query_term, "_", df$gene))==FALSE)
return(df_out)
}
# +++++++++ Process output  +++++++++
strucmodel="cancigalc_constr_corRes"
GWASsumStats=readRDS(paste0(HOME,"/output/rds/", strucmodel, "_Sumstats.rds"))
postGWA=list()
clumpedSNPsList=list()
clumpedSNP_eQTLList=list()
phenoNames=names(GWASsumStats)
# Identify phenotype corresponding to the lowest p-value
# Standardize beta
standardBeta=function(df){
print("Derive standardized beta")
zscore = df$BETA / df$SE
N=df$Neff
df$BETA_STD = zscore / sqrt(N)  # Use effective sample size
df$BETA_STD_pos=sqrt((df$BETA_STD)^2) # make all beta estimates positive
print("Get standard error")
VAR_STD = (1-df$BETA^2)/N
df$SE_STD = sqrt(VAR_STD)
return(df)
}
GWASsumStats=lapply(GWASsumStats, function(x) standardBeta(x))
str(GWASsumStats)
# Identify phenotype corresponding to the lowest p-value
# Standardize beta
standardBeta=function(df){
print("Derive standardized beta")
zscore = df$BETA / df$SE
N=df$Neff
df$BETA_STD = zscore / sqrt(N)  # Use effective sample size
df$BETA_STD_pos=sqrt((df$BETA_STD)^2) # make all beta estimates positive
print("Get standard error")
VAR_STD = (1-df$BETA^2)/N
df$SE_STD = sqrt(VAR_STD)
df$SE_STDTesr = sqrt((1/N))
return(df)
}
GWASsumStats=lapply(GWASsumStats, function(x) standardBeta(x))
str(GWASsumStats)
# Identify phenotype corresponding to the lowest p-value
# Standardize beta
standardBeta=function(df){
print("Derive standardized beta")
zscore = df$BETA / df$SE
N=df$Neff
df$BETA_STD = zscore / sqrt(N)  # Use effective sample size
df$BETA_STD_pos=sqrt((df$BETA_STD)^2) # make all beta estimates positive
print("Get standard error")
VAR_STD = (1-df$BETA^2)/N
df$SE_STD = sqrt(VAR_STD)
df$SE_STDTesr = sqrt((1-df$BETA_STD^2)/N)
return(df)
}
GWASsumStats=lapply(GWASsumStats, function(x) standardBeta(x))
str(GWASsumStats)
# Identify phenotype corresponding to the lowest p-value
# Standardize beta
standardBeta=function(df){
print("Derive standardized beta")
zscore = df$BETA / df$SE
N=df$Neff
df$BETA_STD = zscore / sqrt(N)  # Use effective sample size
df$BETA_STD_pos=sqrt((df$BETA_STD)^2) # make all beta estimates positive
print("Get standard error")
VAR_STD = 1/N
df$SE_STD = sqrt(VAR_STD)
return(df)
}
GWASsumStats=lapply(GWASsumStats, function(x) standardBeta(x))
